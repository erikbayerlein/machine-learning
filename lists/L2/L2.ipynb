{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d9bbd3e",
   "metadata": {},
   "source": [
    "## Erik Bayerlein - 537606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5b818b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795fa05a",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9f4ef",
   "metadata": {},
   "source": [
    "#### Auxiliary Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "98663607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(title, results):\n",
    "    print(title)\n",
    "    print(f\"Accuracy: {results[0]:.2f} ± {results[1]:.2f}\")\n",
    "    print(f\"Accuracy per class: {results[2]}\")\n",
    "    print(f\"Std per class: {results[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fd9e2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "    return (X - means) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3804e8",
   "metadata": {},
   "source": [
    "#### K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2147d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, model, k=10):\n",
    "    fold_size = len(X) // k\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    accuracies = []\n",
    "    accuracies_per_class = {c: [] for c in np.unique(y)}\n",
    "    \n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size if i < k - 1 else len(X)\n",
    "        \n",
    "        test_indices = indices[start:end]\n",
    "        train_indices = np.concatenate([indices[:start], indices[end:]])\n",
    "        \n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        for cls in np.unique(y):\n",
    "            cls_indices = np.where(y_test == cls)\n",
    "            cls_accuracy = np.mean(y_pred[cls_indices] == y_test[cls_indices])\n",
    "            accuracies_per_class[cls].append(cls_accuracy)\n",
    "    \n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    avg_accuracy_per_class = {cls: np.mean(scores) for cls, scores in accuracies_per_class.items()}\n",
    "    std_accuracy_per_class = {cls: np.std(scores) for cls, scores in accuracies_per_class.items()}\n",
    "    \n",
    "    return avg_accuracy, std_accuracy, avg_accuracy_per_class, std_accuracy_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3448bb",
   "metadata": {},
   "source": [
    "#### Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ef53c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiscriminantAnalysis:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.means = np.array([X[y == c].mean(axis=0) for c in self.classes])\n",
    "        self.covariance = np.cov(X.T)\n",
    "        self.priors = np.array([np.mean(y == c) for c in self.classes])\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = np.array([self._pdf(X, mean, self.covariance) for mean in self.means])\n",
    "        return self.classes[np.argmax(probabilities, axis=0)]\n",
    "\n",
    "    def _pdf(self, X, mean, covariance):\n",
    "        cov_inv = np.linalg.inv(covariance)\n",
    "        det_cov = np.linalg.det(covariance)\n",
    "        norm_const = 1.0 / (np.sqrt((2 * np.pi) ** X.shape[1] * det_cov))\n",
    "        X_mean = X - mean\n",
    "        exponent = np.einsum('ij,ji->i', X_mean.dot(cov_inv), X_mean.T)\n",
    "        return norm_const * np.exp(-0.5 * exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ff981",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8b54dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.means = np.array([X[y == c].mean(axis=0) for c in self.classes])\n",
    "        self.vars = np.array([X[y == c].var(axis=0) for c in self.classes])\n",
    "        self.priors = np.array([np.mean(y == c) for c in self.classes])\n",
    "\n",
    "    def predict(self, X):\n",
    "        likelihoods = np.array([self._pdf(X, mean, var) for mean, var in zip(self.means, self.vars)])\n",
    "        posteriors = likelihoods * self.priors[:, np.newaxis]\n",
    "        return self.classes[np.argmax(posteriors, axis=0)]\n",
    "\n",
    "    def _pdf(self, X, mean, var):\n",
    "        exponent = np.exp(- (X - mean) ** 2 / (2 * var))\n",
    "        return exponent.prod(axis=1) / np.sqrt(2 * np.pi * var).prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d3e1d",
   "metadata": {},
   "source": [
    "#### Gradient Descent (GD) for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "944f56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1] + 1)\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        for _ in range(self.epochs):\n",
    "            predictions = self.sigmoid(X.dot(self.theta))\n",
    "            gradient = X.T.dot(predictions - y) / y.size\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        return self.sigmoid(X.dot(self.theta)) >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef49c3",
   "metadata": {},
   "source": [
    "#### Gradient Descent for Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2c914d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.num_classes = len(self.classes)\n",
    "        y = y.astype(int)\n",
    "\n",
    "        self.theta = np.zeros((X.shape[1] + 1, self.num_classes))\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        y_one_hot = np.eye(self.num_classes)[y]\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            scores = X.dot(self.theta)\n",
    "            probs = self._softmax(scores)\n",
    "            gradient = X.T.dot(probs - y_one_hot) / y.size\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "    def _softmax(self, scores):\n",
    "        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        scores = X.dot(self.theta)\n",
    "        probs = self._softmax(scores)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd995e0f",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285b5f6",
   "metadata": {},
   "source": [
    "### Q1 - Logistic Regression, Gaussiam Discriminant Analysis and Gaussian Naive Bayes with Cross Validation in 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6ac02c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../datasets/breastcancer.csv')\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "80f66d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_model = GaussianNaiveBayes()\n",
    "gda_model = GaussianDiscriminantAnalysis()\n",
    "logistic_model = LogisticRegressionGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2eebc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_logistic = k_fold_cross_validation(X, y, logistic_model, k=10)\n",
    "results_gda = k_fold_cross_validation(X, y, gda_model, k=10)\n",
    "results_nb = k_fold_cross_validation(X, y, naive_bayes_model, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9f616366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression GD\n",
      "Accuracy: 0.98 ± 0.01\n",
      "Accuracy per class: {np.float64(0.0): np.float64(0.9922972972972973), np.float64(1.0): np.float64(0.9534490740740742)}\n",
      "Std per class: {np.float64(0.0): np.float64(0.016239285392705603), np.float64(1.0): np.float64(0.04041569220538391)}\n",
      "\n",
      "Gaussian Discriminant Analysis:\n",
      "Accuracy: 0.96 ± 0.03\n",
      "Accuracy per class: {np.float64(0.0): np.float64(0.9944444444444445), np.float64(1.0): np.float64(0.9030725755725756)}\n",
      "Std per class: {np.float64(0.0): np.float64(0.016666666666666673), np.float64(1.0): np.float64(0.06382831867292928)}\n",
      "\n",
      "Naive Bayes Gaussian:\n",
      "Accuracy: 0.93 ± 0.04\n",
      "Accuracy per class: {np.float64(0.0): np.float64(0.9509229225859548), np.float64(1.0): np.float64(0.8997144249512671)}\n",
      "Std per class: {np.float64(0.0): np.float64(0.040545230193575005), np.float64(1.0): np.float64(0.0844156662980928)}\n"
     ]
    }
   ],
   "source": [
    "show_results(title=\"Logistic Regression GD\", results=results_logistic)\n",
    "show_results(title=\"\\nGaussian Discriminant Analysis:\", results=results_gda)\n",
    "show_results(title=\"\\nNaive Bayes Gaussian:\", results=results_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cbd729",
   "metadata": {},
   "source": [
    "### Q2 - Softmax Regression, Gaussiam Discriminant Analysis and Gaussian Naive Bayes with Cross Validation in 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d46fc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../datasets/vehicle.csv')\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "09afb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_model = SoftmaxRegressionGD()\n",
    "gda_model = GaussianDiscriminantAnalysis()\n",
    "naive_bayes_model = GaussianNaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "55137a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_softmax = k_fold_cross_validation(X, y, softmax_model, k=10)\n",
    "results_gda = k_fold_cross_validation(X, y, gda_model, k=10)\n",
    "results_nb = k_fold_cross_validation(X, y, naive_bayes_model, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4ccb46d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression GD\n",
      "Accuracy: 0.71 ± 0.06\n",
      "Accuracy per class: {np.float64(0.0): np.float64(0.8874297249491757), np.float64(1.0): np.float64(0.453252402568192), np.float64(2.0): np.float64(0.5553872977323017), np.float64(3.0): np.float64(0.9638200894392843)}\n",
      "Std per class: {np.float64(0.0): np.float64(0.080555832027033), np.float64(1.0): np.float64(0.1150952110598815), np.float64(2.0): np.float64(0.12033386796419603), np.float64(3.0): np.float64(0.03865456318548034)}\n",
      "\n",
      "Gaussian Discriminant Analysis\n",
      "Accuracy: 0.77 ± 0.04\n",
      "Accuracy per class: {np.float64(0.0): np.float64(0.9473471805953648), np.float64(1.0): np.float64(0.6141621811274813), np.float64(2.0): np.float64(0.5771690247205121), np.float64(3.0): np.float64(0.9596969696969697)}\n",
      "Std per class: {np.float64(0.0): np.float64(0.0322853971193488), np.float64(1.0): np.float64(0.12023529096767378), np.float64(2.0): np.float64(0.1121957036597623), np.float64(3.0): np.float64(0.03653285542574816)}\n",
      "\n",
      "Naive Bayes Gaussian\n",
      "Accuracy: 0.44 ± 0.06\n",
      "Accuracy per class: {np.float64(0.0): np.float64(0.17767676999672447), np.float64(1.0): np.float64(0.3882692650748244), np.float64(2.0): np.float64(0.37507150451887294), np.float64(3.0): np.float64(0.875688240208364)}\n",
      "Std per class: {np.float64(0.0): np.float64(0.08689814288427647), np.float64(1.0): np.float64(0.16079200100457008), np.float64(2.0): np.float64(0.1414853993391049), np.float64(3.0): np.float64(0.06365610221736415)}\n"
     ]
    }
   ],
   "source": [
    "show_results(title=\"Softmax Regression GD\", results=results_softmax)\n",
    "show_results(title=\"\\nGaussian Discriminant Analysis\", results=results_gda)\n",
    "show_results(title=\"\\nNaive Bayes Gaussian\", results=results_nb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
